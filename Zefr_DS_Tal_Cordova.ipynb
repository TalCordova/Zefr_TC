{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TalCordova/Zefr_TC/blob/main/Zefr_DS_Tal_Cordova.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zefr DS Home Assignment\n",
        "\n",
        "Tal Cordova\n",
        "\n",
        "talcordova56@gmail.com\n",
        "\n",
        "0504971013"
      ],
      "metadata": {
        "id": "fJml9BJY-1tL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 - Logo Detection Model"
      ],
      "metadata": {
        "id": "Yn-ZocQR-_D9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### import Data"
      ],
      "metadata": {
        "id": "31K1-nJBZw5A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVJIcGdF-wGy",
        "outputId": "f7f0431e-9e62-4311-b3ea-63fb2aa058bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/drive/MyDrive/Zefr/logo-dataset.v9-raw_images.yolov7pytorch.zip\" -d \"/content/dataset\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DfmARitKBugg",
        "outputId": "af4f1628-9682-488c-aa3e-24f573b8a5f0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open /content/drive/MyDrive/Zefr/logo-dataset.v9-raw_images.yolov7pytorch.zip, /content/drive/MyDrive/Zefr/logo-dataset.v9-raw_images.yolov7pytorch.zip.zip or /content/drive/MyDrive/Zefr/logo-dataset.v9-raw_images.yolov7pytorch.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# View random image\n",
        "import random\n",
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import yaml\n",
        "\n",
        "def show_random_image():\n",
        "    # Read data.yaml to get class names\n",
        "    with open('/content/dataset/data.yaml', 'r') as f:\n",
        "        data = yaml.safe_load(f)\n",
        "        class_names = data['names']\n",
        "\n",
        "    image_dir = '/content/dataset/train/images'\n",
        "    random_image = random.choice(os.listdir(image_dir))\n",
        "    image_path = os.path.join(image_dir, random_image)\n",
        "    label_path = image_path.replace('images', 'labels').replace('.jpg', '.txt')\n",
        "\n",
        "    img = cv2.imread(image_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    with open(label_path, 'r') as f:\n",
        "        label = f.read().strip().split()\n",
        "        class_id = int(label[0])\n",
        "        class_name = class_names[class_id]\n",
        "\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.imshow(img)\n",
        "    plt.title(f\"Label: {class_name}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "show_random_image()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "hSYyotaPIquI",
        "outputId": "64ed5ed6-f955-4fa8-bb89-10cfea5a0c87"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/dataset/data.yaml'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-dd6642ab39c8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mshow_random_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-dd6642ab39c8>\u001b[0m in \u001b[0;36mshow_random_image\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mshow_random_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Read data.yaml to get class names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/dataset/data.yaml'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msafe_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mclass_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/dataset/data.yaml'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert Labels"
      ],
      "metadata": {
        "id": "J4PNT3T_Z2QF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import yaml\n",
        "import shutil\n",
        "\n",
        "# Paths\n",
        "base_dir = \"/content/dataset\"  # Path to the original dataset directory\n",
        "yaml_file = os.path.join(base_dir, \"data.yaml\")  # Path to the data.yaml file\n",
        "output_base_dir = \"/content/dataset_binary\"  # Path to save binary labels and images\n",
        "\n",
        "# Ensure output directories exist for train and valid\n",
        "os.makedirs(os.path.join(output_base_dir, \"train/images\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(output_base_dir, \"train/labels\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(output_base_dir, \"valid/images\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(output_base_dir, \"valid/labels\"), exist_ok=True)\n",
        "\n",
        "# Load the YAML file to map class IDs to brand names\n",
        "with open(yaml_file, \"r\") as file:\n",
        "    data = yaml.safe_load(file)\n",
        "class_names = data[\"names\"]  # List of class names\n",
        "target_brands = {'cocacola', 'coke', 'disney', 'starbucks', 'mcdonalds', 'mcdonalds_text'}\n",
        "\n",
        "# Find class IDs corresponding to the target brands\n",
        "target_class_ids = {i for i, name in enumerate(class_names) if name in target_brands}\n",
        "\n",
        "# Function to process labels and copy images\n",
        "def process_labels_and_images(input_image_dir, input_label_dir, output_image_dir, output_label_dir):\n",
        "    for label_file in os.listdir(input_label_dir):\n",
        "        input_label_path = os.path.join(input_label_dir, label_file)\n",
        "        output_label_path = os.path.join(output_label_dir, label_file)\n",
        "\n",
        "        with open(input_label_path, \"r\") as file:\n",
        "            lines = file.readlines()\n",
        "\n",
        "        # Convert class IDs to binary labels\n",
        "        new_lines = []\n",
        "        for line in lines:\n",
        "            parts = line.strip().split()\n",
        "            class_id = int(parts[0])  # Get the class ID\n",
        "\n",
        "            # Convert to binary label\n",
        "            binary_label = 1 if class_id in target_class_ids else 0\n",
        "\n",
        "            # Replace class_id with binary_label, keeping bbox info\n",
        "            new_line = f\"{binary_label} \" + \" \".join(parts[1:])\n",
        "            new_lines.append(new_line)\n",
        "\n",
        "        # Save the modified labels\n",
        "        with open(output_label_path, \"w\") as file:\n",
        "            file.write(\"\\n\".join(new_lines))\n",
        "\n",
        "        # Copy the corresponding image\n",
        "        image_file = label_file.replace(\".txt\", \".jpg\")\n",
        "        input_image_path = os.path.join(input_image_dir, image_file)\n",
        "        output_image_path = os.path.join(output_image_dir, image_file)\n",
        "\n",
        "        if os.path.exists(input_image_path):\n",
        "            shutil.copy(input_image_path, output_image_path)\n",
        "\n",
        "# Process train and valid datasets\n",
        "process_labels_and_images(\n",
        "    os.path.join(base_dir, \"train/images\"),\n",
        "    os.path.join(base_dir, \"train/labels\"),\n",
        "    os.path.join(output_base_dir, \"train/images\"),\n",
        "    os.path.join(output_base_dir, \"train/labels\")\n",
        ")\n",
        "\n",
        "process_labels_and_images(\n",
        "    os.path.join(base_dir, \"valid/images\"),\n",
        "    os.path.join(base_dir, \"valid/labels\"),\n",
        "    os.path.join(output_base_dir, \"valid/images\"),\n",
        "    os.path.join(output_base_dir, \"valid/labels\")\n",
        ")\n",
        "\n",
        "print(\"Labels and images successfully processed for both train and valid!\")"
      ],
      "metadata": {
        "id": "8_WknRFlVxKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Check image size\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# Paths\n",
        "image_dir = '/content/dataset_binary/train/images'  # Change to valid/images if needed\n",
        "\n",
        "# Check image sizes\n",
        "image_sizes = []\n",
        "for img_file in os.listdir(image_dir):\n",
        "    img_path = os.path.join(image_dir, img_file)\n",
        "    with Image.open(img_path) as img:\n",
        "        image_sizes.append(img.size)  # (width, height)\n",
        "\n",
        "# Print some examples\n",
        "print(f\"Total images: {len(image_sizes)}\")\n",
        "print(\"Sample image sizes:\", image_sizes[:10])\n",
        "\n",
        "# Optional: Find the most common image size\n",
        "from collections import Counter\n",
        "size_counts = Counter(image_sizes)\n",
        "print(\"Most common image sizes:\")\n",
        "for size, count in size_counts.most_common(5):\n",
        "    print(f\"{size}: {count} images\")"
      ],
      "metadata": {
        "id": "9M6z0Mv0juNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Show Random Images"
      ],
      "metadata": {
        "id": "-uQN8aOFZ509"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_random_image_with_simple_label(set_type='train'):\n",
        "    # Paths\n",
        "    image_dir = f'/content/dataset_binary/{set_type}/images'\n",
        "    label_dir = f'/content/dataset_binary/{set_type}/labels'\n",
        "\n",
        "    # Select a random image\n",
        "    random_image = random.choice(os.listdir(image_dir))\n",
        "    image_path = os.path.join(image_dir, random_image)\n",
        "    label_path = os.path.join(label_dir, random_image.replace('.jpg', '.txt'))\n",
        "\n",
        "    # Read the image\n",
        "    img = cv2.imread(image_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Default label\n",
        "    label = \"Unknown\"\n",
        "\n",
        "    # Check the label file\n",
        "    if os.path.exists(label_path):\n",
        "        with open(label_path, 'r') as f:\n",
        "            labels = f.readlines()\n",
        "        # Check if any label is 1\n",
        "        label = \"Belongs (1)\" if any(int(line.split()[0]) == 1 for line in labels) else \"Does Not Belong (0)\"\n",
        "\n",
        "    # Show the image with the label at the top\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(img)\n",
        "    plt.title(f\"Image: {random_image}\\nLabel: {label}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Call the function for train set\n",
        "show_random_image_with_simple_label(set_type='train')\n"
      ],
      "metadata": {
        "id": "jeEDIFB6Wz4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_random_image_with_simple_label(set_type='valid')"
      ],
      "metadata": {
        "id": "a7F1GpaGWAjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Check class counts\n",
        "\n",
        "from collections import Counter\n",
        "import os\n",
        "\n",
        "def count_labels(label_dir):\n",
        "    counts = Counter()\n",
        "    for label_file in os.listdir(label_dir):\n",
        "        with open(os.path.join(label_dir, label_file), \"r\") as f:\n",
        "            for line in f:\n",
        "                class_id = int(line.strip().split()[0])  # Extract the binary class label\n",
        "                counts[class_id] += 1\n",
        "    return counts\n",
        "\n",
        "# Count labels in train and valid datasets\n",
        "train_counts = count_labels(\"/content/dataset_binary/train/labels\")\n",
        "valid_counts = count_labels(\"/content/dataset_binary/valid/labels\")\n",
        "\n",
        "print(f\"Train Label Counts: {train_counts}\")\n",
        "print(f\"Valid Label Counts: {valid_counts}\")"
      ],
      "metadata": {
        "id": "iZL5i6nJzULh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Note**: We can see that the classes are imbalanced, and therefore, will need to consider it while training and evaluating the model"
      ],
      "metadata": {
        "id": "FGDR7Qt-zjbz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build the Model\n",
        "\n",
        "I decided to use TinyVGG architecture for classification because:\n",
        "\n",
        "1. Lightweight Architecture: TinyVGG uses fewer parameters, making it faster to train and deploy.\n",
        "2. Image Classification Focus: Works well for binary classification when combined with a final sigmoid activation.\n",
        "3. Simple Training: Easy to implement and fine-tune for small datasets.\n",
        "\n",
        "I used several augmentations to increase generalization and strenghtehn the model:\n",
        "\n",
        "1. Horizontal flip - Many logos can appear flipped (mirrored views on products, ads, or websites).\n",
        "2. Random Rotation - Logos in real-world scenarios may not always be perfectly aligned (tilted logos on billboards, products, or packaging).\n",
        "3. Resize - We have different image sizes, and since I am using TinyVGG, I will convert all of them to `224x224` image sizes and augment the train set."
      ],
      "metadata": {
        "id": "jJhsLuG3Z8q7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Augmentation and resize\n",
        "from torchvision import transforms\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),           # Resize all images to 224x224\n",
        "    transforms.RandomHorizontalFlip(),       # Randomly flip images horizontally\n",
        "    transforms.RandomRotation(10),          # Random rotation within Â±10 degrees\n",
        "    transforms.ToTensor(),                  # Convert to PyTorch tensor\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "valid_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),          # Resize all images to 224x224\n",
        "    transforms.ToTensor(),                  # Convert to PyTorch tensor\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])"
      ],
      "metadata": {
        "id": "Rz43R4ujWPlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create custom Dataset\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.io import read_image\n",
        "import os\n",
        "import torch\n",
        "\n",
        "class LogoDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, image_dir, label_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.label_dir = label_dir\n",
        "        self.transform = transform\n",
        "        self.images = os.listdir(image_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = os.path.join(self.image_dir, self.images[idx])\n",
        "        label_path = os.path.join(self.label_dir, self.images[idx].replace('.jpg', '.txt'))\n",
        "\n",
        "        # Load the image\n",
        "        image = Image.open(image_path).convert(\"RGB\")  # Ensure image is RGB\n",
        "\n",
        "        # Apply the transform\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Load the label\n",
        "        with open(label_path, 'r') as f:\n",
        "            labels = f.readlines()\n",
        "        label = 1 if any(int(line.split()[0]) == 1 for line in labels) else 0\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "41MUR5ymamZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths\n",
        "train_image_dir = '/content/dataset_binary/train/images'\n",
        "train_label_dir = '/content/dataset_binary/train/labels'\n",
        "valid_image_dir = '/content/dataset_binary/valid/images'\n",
        "valid_label_dir = '/content/dataset_binary/valid/labels'\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = LogoDataset(train_image_dir, train_label_dir, transform=train_transforms)\n",
        "valid_dataset = LogoDataset(valid_image_dir, valid_label_dir, transform=valid_transforms)\n",
        "\n",
        "# Create dataloaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "yt2nTdh1axnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TinyVGG"
      ],
      "metadata": {
        "id": "vBiYRXE2daJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "RH5tTxzcfs7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "id": "gdQy25uAftr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TinyVGG(nn.Module):\n",
        "    def __init__(self, input_channels: int, hidden_units: int, output_shape: int = 1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=input_channels,\n",
        "                               out_channels=hidden_units,\n",
        "                               kernel_size=3,\n",
        "                               stride=1,\n",
        "                               padding=1)  # Keeps input size\n",
        "        self.pool = nn.MaxPool2d(2)  # Halves input size\n",
        "        self.conv2 = nn.Conv2d(in_channels=hidden_units,\n",
        "                               out_channels=hidden_units,\n",
        "                               kernel_size=3,\n",
        "                               stride=1,\n",
        "                               padding=1)\n",
        "\n",
        "        # Placeholder for dynamically calculated input size\n",
        "        self.flatten_input_size = None\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear = None  # Placeholder for the final linear layer\n",
        "\n",
        "        self.output_shape = output_shape\n",
        "        self.hidden_units = hidden_units\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Dynamically initialize the linear layer on the first forward pass\n",
        "        if self.linear is None:\n",
        "            self.flatten_input_size = x.shape[1] * x.shape[2] * x.shape[3]\n",
        "            self.linear = nn.Linear(self.flatten_input_size, self.output_shape).to(x.device)\n",
        "\n",
        "        x = self.flatten(x)\n",
        "        x = self.linear(x)  # Output shape: [batch_size, 1]\n",
        "        return x.squeeze()  # Remove the extra dimension, output shape: [batch_size]"
      ],
      "metadata": {
        "id": "XRZN6jmndbXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_fn(y_true, y_pred):\n",
        "    \"\"\"Calculates accuracy between truth labels and predictions.\n",
        "\n",
        "    Args:\n",
        "        y_true (torch.Tensor): Truth labels for predictions.\n",
        "        y_pred (torch.Tensor): Predictions to be compared to predictions.\n",
        "\n",
        "    Returns:\n",
        "        [torch.float]: Accuracy value between y_true and y_pred, e.g. 78.45\n",
        "    \"\"\"\n",
        "    correct = torch.eq(y_true, y_pred).sum().item()\n",
        "    acc = (correct / len(y_pred)) * 100\n",
        "    return acc\n"
      ],
      "metadata": {
        "id": "9bMeT18rgY6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "def train_step(model: torch.nn.Module,\n",
        "               data_loader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               accuracy_fn,\n",
        "               device: torch.device):\n",
        "    train_loss, train_acc = 0, 0\n",
        "    model.train()\n",
        "\n",
        "    for batch, (X, y) in enumerate(data_loader):\n",
        "        X, y = X.to(device), y.to(device).float()  # Ensure `y` is float for BCEWithLogitsLoss\n",
        "\n",
        "        # 1. Forward pass\n",
        "        y_pred = model(X)  # No sigmoid in the model\n",
        "\n",
        "        # 2. Calculate loss\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # 3. Calculate accuracy\n",
        "        y_pred_label = (torch.sigmoid(y_pred) > 0.5).float()  # Apply sigmoid + threshold\n",
        "        train_acc += accuracy_fn(y_true=y, y_pred=y_pred_label)\n",
        "\n",
        "        # 4. Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 5. Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # 6. Optimizer step\n",
        "        optimizer.step()\n",
        "\n",
        "    train_loss /= len(data_loader)\n",
        "    train_acc /= len(data_loader)\n",
        "    print(f\"Train loss: {train_loss:.5f} | Train acc: {train_acc:.2f}\")\n",
        "\n",
        "    return train_loss, train_acc"
      ],
      "metadata": {
        "id": "OmQtJt7xfl3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step(model: torch.nn.Module,\n",
        "              data_loader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              accuracy_fn,\n",
        "              device: torch.device):\n",
        "    test_loss, test_acc = 0, 0\n",
        "\n",
        "    # Put model in eval mode\n",
        "    model.eval()\n",
        "    # Turn on inference mode\n",
        "    with torch.inference_mode():\n",
        "        for X, y in data_loader:\n",
        "            X, y = X.to(device), y.to(device).float()  # Ensure y is float\n",
        "\n",
        "            # 1. Forward pass\n",
        "            test_pred = model(X)\n",
        "\n",
        "            # 2. Calculate the loss\n",
        "            test_loss += loss_fn(test_pred, y).item()\n",
        "\n",
        "            # 3. Calculate accuracy\n",
        "            test_pred_label = (torch.sigmoid(test_pred) > 0.5).float()  # Threshold predictions\n",
        "            test_acc += accuracy_fn(y_true=y, y_pred=test_pred_label)\n",
        "\n",
        "    test_loss /= len(data_loader)\n",
        "    test_acc /= len(data_loader)\n",
        "    print(f\"Test loss: {test_loss:.5f} | Test acc: {test_acc:.2f}\")\n",
        "    return test_loss, test_acc"
      ],
      "metadata": {
        "id": "9P6rn5PqgrrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the Model"
      ],
      "metadata": {
        "id": "9LYx9R2GxZhx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle the imbalance by using class weights in loss function\n",
        "total_train = train_counts[0] + train_counts[1]\n",
        "class_weights = torch.tensor([\n",
        "    total_train / (2 * train_counts[0]),  # Weight for class 0\n",
        "    total_train / (2 * train_counts[1])   # Weight for class 1\n",
        "], dtype=torch.float32).to(device)"
      ],
      "metadata": {
        "id": "k3sZ3AqJ0Gq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "from torch.optim import Adam\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "model = TinyVGG(input_channels=3, hidden_units=16).to(device)\n",
        "loss_fn = BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
        "optimizer = Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "epochs = 3\n",
        "\n",
        "train_time_start_on_gpu = timer()\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  print(f\"Epoch: {epoch}\\n-------\")\n",
        "  train_step(model = model,\n",
        "             data_loader = train_dataloader,\n",
        "             loss_fn = loss_fn,\n",
        "             optimizer = optimizer,\n",
        "             accuracy_fn = accuracy_fn,\n",
        "             device = device)\n",
        "  test_step(model = model,\n",
        "            data_loader = valid_dataloader,\n",
        "            loss_fn = loss_fn,\n",
        "            accuracy_fn = accuracy_fn,\n",
        "            device = device)\n",
        "\n",
        "train_time_end_on_gpu = timer()\n",
        "total_time = train_time_end_on_gpu - train_time_start_on_gpu\n",
        "print(f\"\\nTrain time on {device}: {total_time:.3f} seconds\")"
      ],
      "metadata": {
        "id": "oYIxOVaGg0HD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "def eval_model(model: torch.nn.Module,\n",
        "               data_loader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               accuracy_fn,\n",
        "               device=torch.device(\"cpu\")):\n",
        "    \"\"\"\n",
        "    Returns a dictionary containing the results of model evaluation on data_loader\n",
        "    \"\"\"\n",
        "    loss, acc = 0, 0\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    with torch.inference_mode():\n",
        "        for X, y in tqdm(data_loader):\n",
        "            # Make data device agnostic\n",
        "            X, y = X.to(device), y.to(device).float()  # Ensure labels are float for BCEWithLogitsLoss\n",
        "\n",
        "            # Make predictions\n",
        "            y_pred = model(X).squeeze()  # Remove extra dimension if necessary\n",
        "\n",
        "            # Accumulate loss\n",
        "            loss += loss_fn(y_pred, y).item()  # Use .item() to avoid accumulating tensors\n",
        "\n",
        "            # Accumulate accuracy\n",
        "            y_pred_label = (torch.sigmoid(y_pred) > 0.5).float()  # Apply sigmoid and threshold\n",
        "            acc += accuracy_fn(y_true=y, y_pred=y_pred_label)\n",
        "\n",
        "        # Scale loss and accuracy by the number of batches\n",
        "        loss /= len(data_loader)\n",
        "        acc /= len(data_loader)\n",
        "\n",
        "    return {\n",
        "        \"model_name\": model.__class__.__name__,  # Only works when model is a class instance\n",
        "        \"model_loss\": loss,\n",
        "        \"model_acc\": acc\n",
        "    }"
      ],
      "metadata": {
        "id": "Ber1IVW2qalV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate the Model"
      ],
      "metadata": {
        "id": "QICzIJ0Nxb1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tinyvgg_results = eval_model(\n",
        "    model = model,\n",
        "    data_loader = valid_dataloader,\n",
        "    loss_fn = loss_fn,\n",
        "    accuracy_fn = accuracy_fn,\n",
        "    device = device\n",
        ")\n",
        "\n",
        "tinyvgg_results"
      ],
      "metadata": {
        "id": "MWfcp7OxqbTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the Model\n",
        "\n",
        "Sva the model in `.pth` format to use for deployment"
      ],
      "metadata": {
        "id": "1I29Mr_S4W_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"model_weights.pth\")"
      ],
      "metadata": {
        "id": "zw0SFx1Vwwb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FYy0xti_dM2U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}